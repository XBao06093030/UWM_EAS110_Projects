{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAS 110 Project Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Please try your best to work them out independently. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Enironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Default Grid World\n",
    "default_world = [[1, 1, 2, 1, 2],\n",
    "                 [2, 1, 7, 1, 5],\n",
    "                 [3, 5, 6, 4, 4],\n",
    "                 [3, 2, 3, 4, 2],\n",
    "                 [2, 1, 8, 5, 0],\n",
    "                 [1, 2, 6, 5, 3]]\n",
    "\n",
    "#default_obstacles = {(2, 2), (3, 2)}  # obstacles in the grid\n",
    "\n",
    "default_actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # actions = [up, down, left, right]\n",
    "\n",
    "default_start=(1, 0)\n",
    "\n",
    "default_goal=(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assume that the robot can only move to right and down.\n",
    "Therefore, actions = [down, right] in this case.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = default_world\n",
    "start = default_start\n",
    "goal = default_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DP_NoRecurrsion(world, start, goal):\n",
    "    #initialization\n",
    "    Rs, Cs, Rg, Cg = start[0], start[1], goal[0], goal[1]\n",
    "    cost = [[float('inf')] * len(world[0]) for _ in range(len(world))]\n",
    "    cost[Rs][Cs] = world[Rs][Cs]\n",
    "    \n",
    "    for i in range(Rs + 1, Rg + 1):\n",
    "        cost[i][Cs] = cost[i - 1][Cs] + world[i][Cs]\n",
    "    for j in range(Cs + 1, Cg + 1):\n",
    "        cost[Rs][j] = cost[Rs][j - 1] + world[Rs][j]\n",
    "      \n",
    "    #traverse the map\n",
    "    for i in range(Rs + 1, Rg + 1):\n",
    "        for j in range(Cs + 1, Cg + 1):\n",
    "            cost[i][j] = min(cost[i - 1][j], cost[i][j - 1]) + world[i][j]\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def reconstruct_path(cost, start, goal):\n",
    "    Rs, Cs, Rg, Cg = start[0], start[1], goal[0], goal[1]\n",
    "    current = [goal[0], goal[1]]\n",
    "    path = []\n",
    "    path.append(current)\n",
    "    \n",
    "    while current != start:\n",
    "        i, j = current[0], current[1]\n",
    "        if cost[i - 1][j] < cost[i][j - 1]:\n",
    "            current = (i - 1, j)\n",
    "        else:\n",
    "            current = (i, j - 1)\n",
    "        path.append(current)  \n",
    "    path.reverse()\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimun Cost:  19\n",
      "Optimal Path:  [(1, 0), (2, 0), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), [4, 4]]\n"
     ]
    }
   ],
   "source": [
    "#Output the Answer\n",
    "cost = DP_NoRecurrsion(world, start, goal)\n",
    "print(\"Minimun Cost: \", cost[goal[0]][goal[1]])\n",
    "\n",
    "path = reconstruct_path(cost, start, goal)\n",
    "print(\"Optimal Path: \", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Questions:\n",
    "1. Did I waste some space in memory? Can you reduce the waste?\n",
    "    Hint: cost = [[0 for _ in range(goal[0] - start[0])] for _ in range(goal[1] - start[1])]\n",
    "    \n",
    "2. Can you rewrite the code using recurrsion? Talk about the benefits and drawbacks of doing so.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dijkstra - Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This time, actions = [up, down, left, right].\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = default_world\n",
    "start = default_start\n",
    "goal = default_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dijkstra(world, start, goal):\n",
    "    \"\"\"\n",
    "    :world: 2D list of costs, world[row][col] is the running cost.\n",
    "    :start: a tuple (row, col) that represents start.\n",
    "    :goal: a tuple (row, col) that represents   goal.\n",
    "    :return: (min_cost, optimal path) \n",
    "    \"\"\"\n",
    "\n",
    "    # Dimensions of the world\n",
    "    rows = len(world)\n",
    "    cols = len(world[0])\n",
    "\n",
    "    # Initialize the cost map\n",
    "    cost = [[float('inf')] * cols for _ in range(rows)]\n",
    "    # Distance to the start is the cost of the start cell itself\n",
    "    cost[start[0]][start[1]] = world[start[0]][start[1]]\n",
    "\n",
    "    # Priority queue for cells to explore; stores tuples of (cost, (row, col))\n",
    "    pq = [(world[start[0]][start[1]], start)]\n",
    "\n",
    "    # Keep track of predecessors to reconstruct path\n",
    "    # predecessor[(r, c)] = (prev_r, prev_c)\n",
    "    predecessor = {start: None}\n",
    "\n",
    "    # Directions for up, down, left, right\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "    while pq:\n",
    "        current_cost, (r, c) = heapq.heappop(pq)\n",
    "\n",
    "        # If we've reached the goal, we can reconstruct the path and return\n",
    "        if (r, c) == goal:\n",
    "            return current_cost, reconstruct_path(predecessor, start, goal)\n",
    "\n",
    "        # If we pop a cell whose distance doesn't match the cost array, skip it \n",
    "        # (it means this entry is outdated).\n",
    "        if current_cost > cost[r][c]:\n",
    "            continue\n",
    "\n",
    "        # Check neighbors\n",
    "        for dr, dc in directions:\n",
    "            nr, nc = r + dr, c + dc\n",
    "            # Validate neighbor is within world bounds\n",
    "            if 0 <= nr < rows and 0 <= nc < cols:\n",
    "                # Cost to move to neighbor is current_cost + cost of neighbor cell\n",
    "                new_cost = current_cost + world[nr][nc]\n",
    "                \n",
    "                # If we found a cheaper path to (nr, nc), update and push to queue\n",
    "                if new_cost < cost[nr][nc]:\n",
    "                    cost[nr][nc] = new_cost\n",
    "                    predecessor[(nr, nc)] = (r, c)\n",
    "                    heapq.heappush(pq, (new_cost, (nr, nc)))\n",
    "\n",
    "    # If goal was never reached, return infinity cost and empty path\n",
    "    return float('inf'), []\n",
    "\n",
    "def reconstruct_path(predecessor, start, goal):\n",
    "    \"\"\"\n",
    "    Reconstruct path from predecessor dictionary.\n",
    "    \"\"\"\n",
    "    path = []\n",
    "    current = goal\n",
    "    while current is not None:\n",
    "        path.append(current)\n",
    "        current = predecessor[current]\n",
    "    path.reverse()  # reverse to get path from start to goal\n",
    "\n",
    "    # Sanity check: if the start of the path is not the actual start, something went wrong\n",
    "    if path[0] != start:\n",
    "        return []\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimun Cost:  18\n",
      "Optimal Path:  [(1, 0), (0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (2, 3), (2, 4), (3, 4), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "cost_optimal, path_optimal = dijkstra(world, start, goal)\n",
    "print(\"Minimun Cost: \", cost_optimal)\n",
    "print(\"Optimal Path: \", path_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid-world environment\n",
    "class GridWorld:\n",
    "    def __init__(self, env, actions, start, goal, obstacles=None):\n",
    "        \"\"\"\n",
    "        env: the grid-world.\n",
    "        start: tuple (row, col) for the start state.\n",
    "        goal: tuple (row, col) for the goal state.\n",
    "        obstacles: set of (row, col) cells that are blocked.\n",
    "        \"\"\"\n",
    "        width = len(env[0])\n",
    "        height = len(env)\n",
    "        nA = len(actions)\n",
    "        \n",
    "        self.env = env\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.actions = actions \n",
    "        self.nA = nA\n",
    "        \n",
    "        # Make sure that start and goal are in the world!\n",
    "        if start[0] < height and start[1] < width:\n",
    "            self.start = start\n",
    "        else:\n",
    "            print(\"Invalid Start\")\n",
    "            \n",
    "        if goal[0] < height and goal[1] < width:\n",
    "            self.goal = goal\n",
    "        else:\n",
    "            print(\"Invalid Goal\")\n",
    "        \n",
    "        \n",
    "        # Define obstacles (if any)\n",
    "        if obstacles is None:\n",
    "            obstacles = set()\n",
    "        self.obstacles = obstacles\n",
    "        \n",
    "        self.reset()  \n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    def valid(self, x_new, y_new):\n",
    "        if x_new < 0 or x_new > self.height - 1 or y_new < 0 or y_new > self.width - 1:\n",
    "            return False\n",
    "        elif self.obstacles and (x_new, y_new) in obstacles:\n",
    "            return False \n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        dx, dy = self.actions[action]\n",
    "\n",
    "        if self.valid(x+dx, y+dy):\n",
    "            x, y = (x+dx, y+dy) \n",
    "            \n",
    "        self.state = (x, y)\n",
    "        reward = self.env[x][y]  \n",
    "        \n",
    "        if self.state == self.goal:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        return self.state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        env: The GridEnv\n",
    "        alpha: learning rate\n",
    "        gamma: discount factor\n",
    "        epsilon: exploration rate for epsilon-greedy\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Construct Q-table\n",
    "        # It'll be overridden later\n",
    "        self.q_table = {}\n",
    "        \n",
    "        # For convenience, let's store a list of all possible (r, c) states\n",
    "        self.all_states = [\n",
    "            (r, c) for r in range(env.height) \n",
    "                    for c in range(env.width) \n",
    "                    if (r, c) not in env.obstacles\n",
    "        ]\n",
    "        \n",
    "        # Initialize Q-values to 0 for all states & actions\n",
    "        for state in self.all_states:\n",
    "            self.q_table[state] = np.zeros(len(env.actions))\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore\n",
    "            return np.random.randint(len(self.env.actions))\n",
    "        else:\n",
    "            # Exploit\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"Q-learning update rule.\"\"\"\n",
    "\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        target = reward + self.gamma * self.q_table[next_state][next_action]\n",
    "        \n",
    "        # SARSA-learning update\n",
    "        self.q_table[state][action] = current_q + self.alpha * (target - current_q)\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"Returns the best action for a given state based on current Q-values.\"\"\"\n",
    "        return np.argmax(self.q_table[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa_learning(env, agent, num_episodes=10, max_steps=50):\n",
    "    rewards_history = []\n",
    "    optimal_path = []\n",
    "    max_reward = -100000\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        #reset environment\n",
    "        state = env.reset()\n",
    "        #initial action\n",
    "        action = agent.choose_action(state)\n",
    "        \n",
    "        total_reward = 0\n",
    "        path = []\n",
    "        \n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            path.append((state, action, next_state, total_reward, agent.q_table[state][action]))        \n",
    "            \n",
    "            if done:\n",
    "                agent.q_table[state][action] += agent.alpha * (reward - agent.q_table[state][action])\n",
    "                break\n",
    "                \n",
    "            next_action = agent.choose_action(next_state)\n",
    "            agent.update(state, action, reward, next_state, next_action, done)\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "                \n",
    "        if total_reward > max_reward:\n",
    "            max_reward = total_reward\n",
    "            optimal_path = path\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "    \n",
    "    return rewards_history, optimal_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 0), 2, (1, 0), -2, -14.073762391247348),\n",
       " ((1, 0), 0, (0, 0), -3, -12.42987802772197),\n",
       " ((0, 0), 3, (0, 1), -4, -12.679809982667015),\n",
       " ((0, 1), 0, (0, 1), -5, -12.453910556170865),\n",
       " ((0, 1), 0, (0, 1), -6, -12.429371450609157),\n",
       " ((0, 1), 3, (0, 2), -8, -13.745388662921478),\n",
       " ((0, 2), 3, (0, 3), -9, -11.904778076794631),\n",
       " ((0, 3), 1, (1, 3), -10, -13.632720482597133),\n",
       " ((1, 3), 1, (2, 3), -14, -12.969442195727368),\n",
       " ((2, 3), 1, (3, 3), -18, -10.352193954151176),\n",
       " ((3, 3), 3, (3, 4), -20, -6.995477781015412),\n",
       " ((3, 4), 3, (3, 4), -22, -5.1510744806696565),\n",
       " ((3, 4), 1, (4, 4), -22, 0.0)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define the Grid World\n",
    "world = default_world\n",
    "start = default_start\n",
    "goal = default_goal\n",
    "actions = default_actions\n",
    "#obstacles = {(2, 2), (3, 2)}  # Example obstacles in the grid\n",
    "\n",
    "#CHANGE THE WORLD for minimum cost path\n",
    "world = -1 * np.array(world)\n",
    "\n",
    "env = GridWorld(world, actions, start, goal, obstacles=None)\n",
    "\n",
    "# 2. Create Q-learning agent\n",
    "agent = SarsaAgent(env, alpha=0.1, gamma=0.9, epsilon=0.8)\n",
    "\n",
    "\n",
    "# 3. Train\n",
    "rewards_history, path = train_sarsa_learning(env, agent, num_episodes=500, max_steps=50)\n",
    "\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        env: The GridEnv\n",
    "        alpha: learning rate\n",
    "        gamma: discount factor\n",
    "        epsilon: exploration rate for epsilon-greedy\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Construct Q-table\n",
    "        # It'll be overridden later\n",
    "        self.q_table = {}\n",
    "        \n",
    "        # For convenience, let's store a list of all possible (r, c) states\n",
    "        self.all_states = [\n",
    "            (r, c) for r in range(env.height) \n",
    "                    for c in range(env.width) \n",
    "                    if (r, c) not in env.obstacles\n",
    "        ]\n",
    "        \n",
    "        # Initialize Q-values to 0 for all states & actions\n",
    "        for state in self.all_states:\n",
    "            self.q_table[state] = np.zeros(len(env.actions))\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore\n",
    "            return np.random.randint(len(self.env.actions))\n",
    "        else:\n",
    "            # Exploit\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Q-learning update rule.\"\"\"\n",
    "\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            # No future state to consider\n",
    "            target = reward\n",
    "        else:\n",
    "            # Max over next state's Q-values\n",
    "            # Can help develop immitation learning by replaying the previous experience\n",
    "            \n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Q-learning update\n",
    "        self.q_table[state][action] = current_q + self.alpha * (target - current_q)\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"Returns the best action for a given state based on current Q-values.\"\"\"\n",
    "        return np.argmax(self.q_table[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, agent, num_episodes=10, max_steps=50):\n",
    "    rewards_history = []\n",
    "    optimal_path = []\n",
    "    max_reward = -100000\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        path = []\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            total_reward += reward\n",
    "            \n",
    "            path.append((state, action, next_state, total_reward, agent.q_table[state][action]))        \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        if total_reward > max_reward:\n",
    "            max_reward = total_reward\n",
    "            optimal_path = path\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "    \n",
    "    return rewards_history, optimal_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 0), 0, (0, 0), -1, -9.778342948357613),\n",
       " ((0, 0), 3, (0, 1), -2, -9.774189183412396),\n",
       " ((0, 1), 3, (0, 2), -4, -10.411493512515518),\n",
       " ((0, 2), 3, (0, 3), -5, -9.363153987948184),\n",
       " ((0, 3), 1, (1, 3), -6, -9.29624353850725),\n",
       " ((1, 3), 1, (2, 3), -10, -9.219757102548096),\n",
       " ((2, 3), 1, (3, 3), -14, -5.799942413340491),\n",
       " ((3, 3), 3, (3, 4), -16, -1.9999998004122235),\n",
       " ((3, 4), 1, (4, 4), -16, 0.0)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define the Grid World\n",
    "world = default_world\n",
    "start = default_start\n",
    "goal = default_goal\n",
    "actions = default_actions\n",
    "#CHANGE THE WORLD for minimum cost path\n",
    "world = -1 * np.array(world)\n",
    "\n",
    "env = GridWorld(world, actions, start=(1, 0), goal=(4, 4), obstacles=None)\n",
    "\n",
    "# 2. Create Q-learning agent\n",
    "agent = QLearningAgent(env, alpha=0.1, gamma=0.9, epsilon=0.8)\n",
    "\n",
    "\n",
    "# 3. Train\n",
    "rewards_history, path = train_q_learning(env, agent, num_episodes=500, max_steps=50)\n",
    "\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
